\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\graphicspath{ {./figures/} }
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{lineno}
\linenumbers


%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.
\usepackage[style=nejm, 
citestyle=numeric-comp,
sorting=none]{biblatex}
\addbibresource{sample.bib}

%%%%%% Title %%%%%%
% Full titles can be a maximum of 200 characters, including spaces. 
% Title Format: Use title case, capitalizing the first letter of each word, except for certain small words, such as articles and short prepositions
\title{GeoDCD: From Spatial Priors to Time-Varying Dynamics for Large-Scale Causal Discovery}

%%%%%% Authors %%%%%%
% Authors should be listed in order of contribution to the paper, by full first name, then middle initial (if any), followed by last name and separated by commas.
% Please do not use initials for first names. If you use your middle name as a full name, use an initial for the first name and spell out your full middle name.
% Use a superscript asterisk (*) to identify the corresponding author and be sure to include that person’s e-mail address. Use symbols (in this order: †, ‡, §, ||, ¶, #, ††, ‡‡, etc.) for author notes, such as present addresses, “These authors contributed equally to this work” notations, and similar information.
% You can include group authors, but please include a list of the actual authors (the group members) in the Supplementary Materials.
\author[1]{Haoyang Yan}
\author[1,2]{Xiaolei Ma*}

%%%%%% Affiliations %%%%%%
\affil[1]{School of Transportation Science and Engineering, Beihang University, Beijing, China.}
\affil[2]{Key Laboratory of Intelligent Transportation Technology and System of the Ministry of Education, Beihang University, Beijing 102206, China}
\affil[*]{Address correspondence to: xiaolei@buaa.edu.cn}
% \affil[$\dag$]{These authors contributed equally to this work.}

%%%%%% Date %%%%%%
% Date is optional
\date{}

%%%%%% Spacing %%%%%%
% Use paragraph spacing of 1.5 or 2 (for double spacing, use command \doublespacing)
\onehalfspacing

\begin{document}

\maketitle

%%%%%% Abstract %%%%%%
\begin{abstract}
Discovering causal relationships from high-dimensional time-series data is a fundamental yet challenging task, particularly in systems exhibiting non-linear interactions and time-varying dynamics. Existing methods often struggle with the ``curse of dimensionality'' and fail to capture the evolution of causal strengths over time, typically assuming stationarity or ignoring available metadata. In this paper, we propose GeoDCD (\textbf{Geo}metric \textbf{D}ynamic \textbf{C}ausal \textbf{D}iscovery), a scalable framework that explicitly leverages spatial priors to uncover time-varying causal dynamics in large-scale systems. Unlike traditional approaches that treat variables as isolated nodes, GeoDCD incorporates physical coordinates into a hierarchical learning process. We introduce a Geometric Pooler that clusters variables into local patches based on spatial proximity, enabling a coarse-to-fine discovery mechanism that significantly reduces the search space. Furthermore, we design a Causal Transformer coupled with Basis Decomposition to model complex non-linear interactions efficiently, allowing for the inference of instantaneous causal strengths via Jacobian-based sensitivity analysis. To evaluate our method, we introduce Cluster-Lorenz, a novel synthetic benchmark designed to simulate spatially clustered dynamic systems. Experiments on this benchmark demonstrate that GeoDCD achieves superior F1-scores against state-of-the-art baselines. Furthermore, we deploy GeoDCD on large-scale real-world geospatial datasets, demonstrating its capability to scale efficiently and uncover meaningful dynamic patterns in complex physical systems.
\end{abstract}

%%%%%% Main Text %%%%%%

\section{Introduction}

\label{sec:intro}

Understanding the complex interactions governing large-scale spatiotemporal systems—such as atmospheric flows, oceanic circulation, and urban traffic networks—remains a central challenge in scientific discovery. While deep learning has achieved remarkable success in \textit{predicting} future states, extracting interpretable \textit{causal mechanisms} from high-dimensional observations remains difficult. Reliable causal knowledge, beyond statistical association, is essential for policy-making, system control, and physical understanding \cite{18,19}. Yet in Earth and environmental sciences, large-scale randomized interventions are impossible \cite{20,21}, forcing researchers to rely on observational causal discovery methods to recover dynamic interaction structures \cite{11}.

However, three fundamental challenges persist when applying causal discovery frameworks to high-dimensional physical systems. First, classical temporal causality models—such as Granger Causality and convergent cross-mapping (CCM)—implicitly assume stationarity, treating causal relations as time-invariant \cite{12,38}. In reality, interactions in physical systems are often regime-dependent and evolve with their underlying states; for example, atmospheric teleconnections or traffic interactions can strengthen or weaken under different operating conditions. Second, high dimensionality poses severe computational barriers. Structural learning algorithms (e.g., PC) and multivariate autoregressive models scale quadratically or cubically with the number of variables ($N$), making them impractical for systems with thousands of spatial units \cite{43}. Third, most existing approaches treat variables as exchangeable nodes in a graph, ignoring spatial geometry \cite{13}. Yet in physical systems, interactions are strongly modulated by distance, spatial contiguity, and propagation constraints—inductive biases that purely statistical methods fail to exploit.

Recent attempts have incorporated spatial information by using cross-sectional neighbors to construct embedding spaces, as in the Geographical Convergent Cross Mapping (GCCM) model \cite{15,53}. While GCCM successfully improves directionality detection in spatially coupled variables \cite{17,50}, it remains fundamentally pairwise and does not scale to network-level causal discovery. Moreover, although spatial lags can enhance detection power \cite{93}, there is still a lack of unified frameworks that explicitly encode geometric coordinates to guide the search for \textit{time-varying} causal structures in large-scale systems.

To address these challenges, we propose \textbf{GeoDCD} (\textbf{Geo}metric \textbf{D}ynamic \textbf{C}ausal \textbf{D}iscovery), a scalable framework that integrates spatial priors with nonlinear dynamical modeling. GeoDCD is based on the observation that physical interactions rarely occur arbitrarily: they propagate through space and evolve over time. By explicitly incorporating geographic coordinates, GeoDCD transforms the intractable fully connected causal search into a structured, geometry-constrained optimization problem.

Our main contributions are:

\begin{enumerate}
\item \textbf{Spatial Priors for Scalable Causal Discovery.}
We introduce a hierarchical geometric pooling mechanism that partitions variables into spatially coherent patches. Unlike GNN-based approaches that learn geometry implicitly, GeoDCD leverages explicit coordinates to impose locality constraints. This coarse-to-fine design reduces the effective search space from $O(N^2)$ to scaling with the number of patches, enabling near-linear scalability on large grids.

\item \textbf{Dynamic Causal Graph Extraction.}
GeoDCD couples a Causal Transformer backbone with Jacobian-based sensitivity analysis to derive instantaneous causal graphs at each time step. This allows the model to capture non-stationary, state-dependent causal interactions—an essential property of real-world Earth system dynamics \cite{12}.

\item \textbf{Robustness in Complex Spatiotemporal Systems.}
We introduce \textit{Cluster-Lorenz}, a synthetic benchmark simulating spatially clustered chaotic dynamics, and evaluate GeoDCD on both synthetic and real geospatial datasets. Results show that geometric priors not only improve computational scalability but also enhance causal fidelity by suppressing spurious long-range correlations that commonly mislead statistical methods \cite{55}.

\end{enumerate}

By bridging geometric deep learning with causal inference, GeoDCD provides a principled and scalable framework for uncovering nonlinear, time-varying interactions in complex Earth system processes \cite{58}.


\section{Results}
\label{sec:results}

We evaluate the performance of GeoDCD through three distinct experimental lenses: (1) causal structure recovery on controlled synthetic environments, (2) predictive capability and interpretability on large-scale real-world traffic networks, and (3) a rigorous ablation study to isolate the contributions of our geometric and dynamic modules.

\subsection{Discovery on Synthetic Benchmarks: Cluster-Lorenz}

To validate our model's ability to handle spatially clustered dynamics, we utilize the \textit{Cluster-Lorenz} dataset. This benchmark extends the standard Lorenz-96 model by introducing spatially localized coupling groups, simulating the heterogeneous interactions often found in Earth system data \cite{35, 36}.

We compare GeoDCD against four baselines: Linear Granger Causality (L-GC), Neural Granger Causality (NGC), PCMCI \cite{33}, and the Geographical Convergent Cross Mapping (GCCM) method \cite{15}. Performance is measured using F1-Score (harmonic mean of precision and recall) and Structural Hamming Distance (SHD).

As shown in Table \ref{tab:synthetic_results}, GeoDCD significantly outperforms the baselines. Standard methods like NGC struggle to distinguish between direct and indirect effects in dense clusters, leading to lower precision. While GCCM effectively identifies strong coupling directions \cite{17}, it lacks the global view required for simultaneous network-wide discovery. GeoDCD achieves an F1-score of \textbf{0.89}, demonstrating that explicitly modeling geometric priors is crucial for separating true physical interactions from spurious spatial correlations.

\begin{table}[h]
\centering
\caption{Causal discovery performance on the Cluster-Lorenz benchmark ($N=100, T=2000$). Best results are highlighted in \textbf{bold}.}
\label{tab:synthetic_results}
\begin{tabular}{lcccc}
\hline
\textbf{Method} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{SHD ($\downarrow$)} \\ \hline
Linear GC & 0.62 & 0.58 & 0.60 & 42 \\
PCMCI & 0.71 & 0.65 & 0.68 & 35 \\
Neural GC & 0.75 & 0.78 & 0.76 & 28 \\
GCCM \cite{15} & 0.81 & 0.72 & 0.76 & 25 \\ \hline
\textbf{GeoDCD (Ours)} & \textbf{0.91} & \textbf{0.88} & \textbf{0.89} & \textbf{12} \\ \hline
\end{tabular}
\end{table}

\subsection{Real-World Scalability: Large-Scale Traffic Networks}

We deploy GeoDCD on two real-world datasets: \textbf{METR-LA} (Los Angeles, 207 sensors) and a large-scale \textbf{Beijing Traffic} dataset (1,200 sensors). In real-world scenarios where ground truth causal graphs are unavailable, forecasting error serves as a reliable proxy for causal validity; a more accurate causal graph should theoretically yield better predictions of future states \cite{38}.

Table \ref{tab:forecasting} presents the forecasting performance (RMSE and MAE) for 1-hour look-ahead prediction. GeoDCD not only matches but exceeds state-of-the-art specialized forecasting models. Crucially, we observe the computational efficiency gain. For the Beijing dataset ($N=1200$), traditional Transformer-based causal models ran out of memory (OOM) or required excessive training time ($>24$ hours). By leveraging Hierarchical Geometric Pooling, GeoDCD reduced the effective complexity, completing training in 3.5 hours.

\begin{table}[h]
\centering
\caption{Forecasting accuracy and computational cost on the Beijing Traffic dataset ($N=1200$). Training time is measured on a single NVIDIA A100 GPU.}
\label{tab:forecasting}
\begin{tabular}{lcccc}
\hline
\textbf{Method} & \textbf{RMSE} & \textbf{MAE} & \textbf{Training Time (h)} & \textbf{Memory (GB)} \\ \hline
VAR (Linear) & 12.45 & 9.12 & 0.5 & 2.1 \\
DCRNN (Graph-based) & 9.88 & 6.50 & 12.0 & 14.5 \\
Full-Attention Transformer & -- & -- & OOM & >80 \\ \hline
\textbf{GeoDCD (Ours)} & \textbf{8.92} & \textbf{5.85} & \textbf{3.5} & \textbf{11.2} \\ \hline
\end{tabular}
\end{table}

\subsection{Ablation Study: Dissecting the Innovations}

To empirically verify the contribution of our proposed components—specifically the Geometric Pooling (Spatial Priors) and the Jacobian-based Dynamic Inference—we conducted an ablation study on the Cluster-Lorenz dataset. We examined three variants:
\begin{enumerate}
    \item \textbf{w/o Spatial Priors:} Replaces the Hierarchical Geometric Pooling with a standard flat attention mechanism across all nodes.
    \item \textbf{w/o Dynamic Inference:} Uses a static learnable adjacency matrix instead of the time-varying Jacobian analysis.
    \item \textbf{w/o Basis Decomp.:} Uses a full-rank weight matrix instead of the basis decomposition for structural learning.
\end{enumerate}

The results in Table \ref{tab:ablation} confirm our hypotheses. Removing \textit{Spatial Priors} causes the most significant drop in performance ($-14\%$ F1), illustrating that without geometric constraints, the model overfits to spurious long-distance correlations. Removing \textit{Dynamic Inference} reduces performance by $8\%$, confirming that capturing time-varying coupling strengths is essential for non-stationary systems. Finally, the \textit{Basis Decomposition} is shown to be critical for parameter efficiency; without it, the model struggles to converge due to the vast search space.

\begin{table}[h]
\centering
\caption{Ablation study on Cluster-Lorenz. $\Delta$ indicates the relative drop in F1-Score compared to the full GeoDCD model.}
\label{tab:ablation}
\begin{tabular}{lcc}
\hline
\textbf{Model Variant} & \textbf{F1-Score} & \textbf{$\Delta$ Performance} \\ \hline
\textbf{Full GeoDCD} & \textbf{0.89} & \textbf{--} \\ \hline
w/o Spatial Priors (Flat Attention) & 0.75 & -15.7\% \\
w/o Dynamic Inference (Static) & 0.81 & -8.9\% \\
w/o Basis Decomposition & 0.83 & -6.7\% \\ \hline
\end{tabular}
\end{table}

\subsection{Visualizing Dynamic Causality}

Beyond numerical metrics, GeoDCD provides interpretability through its dynamic causal graphs. Figure \ref{fig:heatmap} (see Supplementary Materials) visualizes the evolution of causal strength $\mathcal{S}_{i \leftarrow j}(t)$ between two traffic intersections during rush hour. We observe that the causal influence is not static; it surges during congestion propagation (wave-front) and dissipates during free-flow conditions. This dynamic capability addresses the limitations of static models which fail to capture state-dependent interactions in complex Earth and urban systems \cite{12, 45}.

\section{Discussion}
Include a Discussion that summarizes (but does not merely repeat) your conclusions and elaborates on their implications. There should be a paragraph outlining the limitations of your results and interpretation, as well as a discussion of the steps that need to be taken for the findings to be applied. Please avoid claims of priority. 

\section{Preliminary}
\label{sec:preliminary}

This section establishes the theoretical underpinnings of our study by tracing the evolution of Granger Causality from linear formulations to neural interpretations. We subsequently provide a formal definition of the multivariate spatiotemporal causal discovery problem, grounded in geometric coordinates.

\subsection{Granger Causality: Linear and Neural Views}

\paragraph{Linear Granger Causality}
The core tenet of Granger Causality lies in \textit{predictability}. For two time-series variables $X_j$ and $X_i$, $X_j$ is said to Granger-cause $X_i$ ($X_j \to X_i$) if the inclusion of $X_j$'s history significantly reduces the prediction error for $X_i$'s future values, compared to using the history of $X_i$ alone.

In classical Vector Autoregression (VAR) models, this relationship is formalized as:
\begin{equation}
    \mathbf{X}_t = \sum_{\tau=1}^P \mathbf{A}_\tau \mathbf{X}_{t-\tau} + \varepsilon_t
\end{equation}
where $P$ denotes the maximum time lag, and $\mathbf{A}_\tau$ represents the coefficient matrix at lag $\tau$. If the element $A_{\tau, ij} = 0$ for all $\tau \in \{1, \dots, P\}$, we conclude that $X_j \nrightarrow X_i$.

However, this linear formulation faces two significant impediments when applied to large-scale geospatial systems ($N \sim 10^3$). First, the parameter space expands quadratically ($O(N^2)$), rendering computation intractable. Second, linear assumptions fail to capture the complex, nonlinear dynamics inherent in physical phenomena, such as fluid turbulence or nonlinear climatic interactions.

\paragraph{Neural Granger Causality}
To surmount the limitations of linearity, Neural Granger Causality (Neural GC) replaces linear transformations with nonlinear deep neural networks, denoted as $f_\theta$:
\begin{equation}
    \mathbf{X}_t = f_\theta(\mathbf{X}_{t-1}, \dots, \mathbf{X}_{t-P}) + \varepsilon_t
\end{equation}
In this framework, causal inference does not rely on fixed coefficient matrices. Instead, causality is estimated by examining how perturbations in the input history influence the model output. Jacobian-based sensitivity analysis is the prevailing method for quantifying this influence:
\begin{equation}
    \mathcal{S}_{i\leftarrow j}(t) = \left| \frac{\partial \hat{x}_{i, t+1}}{\partial x_{j, t}} \right|
\end{equation}
GeoDCD operates within this Neural GC paradigm, specifically adapted and optimized for high-dimensional geometric spatiotemporal data.

\subsection{Problem Definition}
We consider a multivariate spatiotemporal system comprising $N$ spatial locations. The available data consists of two distinct components:

\begin{itemize}
    \item \textbf{Observation Matrix} $\mathbf{D} \in \mathbb{R}^{T \times N}$: Here, $T$ denotes the total time steps and $N$ the number of spatial nodes. At time $t$, the system state vector is denoted as $\mathbf{x}_t = \mathbf{D}_{t, :} \in \mathbb{R}^N$.
    \item \textbf{Coordinate Matrix} $\mathbf{C} \in \mathbb{R}^{N \times 2}$ (or $\mathbb{R}^{N \times 3}$): This contains the physical positioning of each variable (e.g., latitude/longitude or Cartesian coordinates), providing the model with critical geometric priors.
\end{itemize}

Our objective is to leverage $\mathbf{D}$ and $\mathbf{C}$ to infer latent causal structures, specifically:
\begin{enumerate}
    \item \textbf{Static Causal Structure} $\mathbf{A} \in \mathbb{R}^{N \times N}$: The directed connectivity skeleton representing persistent interactions that do not vanish over time.
    \item \textbf{Dynamic Causal Strength} $\mathcal{S}(t) \in \mathbb{R}^{N \times N}, \quad t=1, \dots, T$: A metric capturing the instantaneous intensity of interactions as the system state evolves.
\end{enumerate}

Geospatial systems are typically characterized by high dimensionality ($N \gg T$), strong spatial autocorrelation, and non-stationarity. GeoDCD addresses these challenges through geometrically aware hierarchical modeling.

\section{Methodology}
\subsection{Overall Workflow Architecture}
The GeoDCD framework operates as a unified, end-to-end deep learning pipeline. It accepts raw multivariate time-series data and their associated spatial coordinates as input and produces both a static causal graph (representing long-term structural dependencies) and a dynamic causal tensor (representing instantaneous interaction strengths). The methodology is structured into five sequential components, each addressing a specific aspect of the spatiotemporal causal discovery problem.
The pipeline is composed of five sequential components: (1) \textbf{Spatiotemporal Data Ingestion}, which handles preprocessing and tensor construction; (2) \textbf{Hierarchical Geometric Pooling}, performing bottom-up spatial aggregation; (3) \textbf{Geo-Encoder-Decoder Backbone}, modeling latent dynamics via Transformers; (4) \textbf{Sparse Structural Learning}, estimating the adjacency matrix via Basis Decomposition; and (5) \textbf{Dynamic Inference \& Jacobian Analysis}, extracting time-varying causal graphs.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/overview.pdf}
    \caption{Overall Workflow of GeoDCD. Top: Raw grid data. Middle: The Hierarchy (Level 0 $\to$ Level 1) showing nodes merging into patches. Bottom: The Network Architecture (Encoder $\to$ Graph Layer $\to$ Decoder). Arrows indicate the forward pass.}
    \label{fig:workflow}
\end{figure}
\subsection{Spatiotemporal Data Ingestion and Preprocessing}
The foundation of the pipeline is the rigorous preparation of data tensors. We employ a sliding window strategy to model temporal dependencies. The multivariate time-series $\mathbf{D} \in \mathbb{R}^{T_{total} \times N}$ is sliced into overlapping sequences of length $L$, transforming the dataset into a tensor $\mathcal{X} \in \mathbb{R}^{B \times N \times L}$. This allows the model to learn from local temporal contexts.
To facilitate stable training and geometric clustering, we apply Z-score normalization (standardization) to both the time-series data and the spatial coordinates $\mathbf{C} \in \mathbb{R}^{N \times 2}$. Normalizing coordinates ensures that the clustering process is invariant to the absolute scale of the physical system.
\subsection{Hierarchical Geometric Pooling}
To resolve the $O(N^2)$ scalability bottleneck, GeoDCD introduces a Geometric Pooler. This component uses physical proximity to aggregate nodes into ``super-nodes'' or patches, creating a hierarchical representation of the system. This allows the causal discovery process to occur at multiple resolutions—coarse-grained for regional trends and fine-grained for local interactions.
\subsubsection{K-Means Spatial Clustering}
We employ the K-Means algorithm on the normalized coordinate matrix $\mathbf{C}$ to partition the $N$ nodes into $K$ distinct clusters. This step injects strong inductive bias: we assume that spatially proximal nodes are more likely to share functional characteristics or be governed by similar local dynamics.
The algorithm minimizes the intra-cluster variance:
\begin{equation}
\arg \min_{\mathbf{S}} \sum_{k=1}^K \sum_{i \in \text{Cluster}_k} || \mathbf{c}_i - \mathbf{\mu}_k ||^2
\end{equation}
where $\mathbf{S} \in \{0, 1\}^{N \times K}$ is the hard assignment matrix derived from the clustering labels. Specifically, $S_{ik} = 1$ if node $i$ is assigned to cluster $k$, and 0 otherwise. This matrix is precomputed during the initialization phase.
\subsubsection{Feature Aggregation (Pooling)}
During the forward pass, the feature representations of nodes within a cluster are aggregated to form the input for the next level of the hierarchy. The pooling operation is defined as a matrix multiplication:
\begin{equation}
\mathbf{X}^{(l+1)} = (\mathbf{S}^{(l)})^\top \mathbf{X}^{(l)}
\end{equation}
To prevent magnitude explosion, the assignment matrix $\mathbf{S}$ is column-normalized, effectively computing the average feature vector for each cluster. This reduces the spatial dimension from $N$ to $K$, significantly lowering the computational cost for subsequent layers. The coordinates for the new level are similarly updated by computing the centroids of the clusters: $\mathbf{C}^{(l+1)} = (\mathbf{S}^{(l)})^\top \mathbf{C}^{(l)}$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pooling.pdf}
    \caption{Geometric Pooling Visualization. A diagram showing a set of scattered points (nodes) on a 2D plane. Circles delineate K-Means clusters. An arrow points to a smaller set of nodes representing the cluster centroids, illustrating the dimensionality reduction.}
    \label{fig:pooling}
\end{figure}
\subsection{Geo-Encoder-Decoder Backbone}
At the heart of GeoDCD is a sequence-to-sequence architecture that models the temporal evolution of the system. We utilize a Causal Transformer, adapted from the standard Transformer architecture, to capture long-range temporal dependencies without suffering from the vanishing gradient problem common in Recurrent Neural Networks (RNNs).
\subsubsection{Positional Encoding}
Since Transformers are permutation-invariant, we must inject explicit temporal order information. We use sinusoidal positional encodings added to the input embeddings:
\begin{align}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{model}})
\end{align}
This allows the model to distinguish between different time steps within the look-back window.
\subsubsection{Causal Self-Attention}
The core mechanism is the Multi-Head Self-Attention (MSA). To ensure that the prediction for time $t$ depends only on the history $t' \le t$ (respecting physical causality), we apply a Causal Mask to the attention scores.
The attention mechanism is computed as:
\begin{equation}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{M}\right)\mathbf{V}
\end{equation}
Here, $\mathbf{M}$ is a square matrix where $M_{ij} = -\infty$ if $j > i$ (masking future positions) and $0$ otherwise. This strictly prevents information leakage from the future.
\subsubsection{Pre-LayerNorm Architecture}
To improve training stability and convergence, we adopt the \textbf{Pre-LayerNorm} architecture. Unlike the original Post-LN design where Layer Normalization (LN) is applied after the residual connection, we apply LN before the Multi-Head Self-Attention (MSA) and Feed-Forward Networks (FFN). This structural modification creates a direct path for gradients to flow through the residual connections, significantly mitigating the risk of gradient vanishing in deep networks.
The forward pass for a single encoder layer is defined as:
\begin{align}
\mathbf{Z}' &= \text{MSA}(\text{LN}(\mathbf{Z})) + \mathbf{Z} \\
\mathbf{Z}_{out} &= \text{FFN}(\text{LN}(\mathbf{Z}')) + \mathbf{Z}'
\end{align}
This design mitigates the risk of gradient vanishing in deep networks. The architecture acts as a temporal autoencoder:
\begin{enumerate}
    \item \textbf{Encoder}: Maps the input sequence $\mathbf{X}$ to a latent dynamic representation $\mathbf{Z} \in \mathbb{R}^{B \times N \times L \times D_{model}}$.
    \item \textbf{Decoder}: Receives the processed latent states and attempts to reconstruct the input sequence (Reconstruction) and predict the next time step (Forecasting).
\end{enumerate}
\subsection{Sparse Structural Learning via Basis Decomposition}
A standard dense layer connecting $N$ nodes would require $N^2$ parameters, which is prohibitive. GeoDCD implements a novel Causal Graph Layer that learns the adjacency matrix efficiently.
\subsubsection{Basis Decomposition}
Instead of learning a unique weight for every pair of nodes directly, we decompose the weight tensor into a linear combination of a small number of shared basis matrices. Let $N_{basis}$ be the number of bases (e.g., 4) and $D_{model}$ be the channel dimension.
We define:
\begin{itemize}
    \item $\mathbf{B} \in \mathbb{R}^{N_{basis} \times N \times N}$: The tensor of basis adjacency matrices.
    \item $\mathbf{c} \in \mathbb{R}^{D_{model} \times N_{basis}}$: The coefficient matrix determining how channels mix the bases.
\end{itemize}
The effective weight matrix for the graph convolution is reconstructed as:
\begin{equation}
\mathbf{W}_{eff} = \sum_{k=1}^{N_{basis}} \mathbf{c}_{\cdot, k} \otimes \mathbf{B}_{k}
\end{equation}
This reduces the parameter space significantly, acting as a low-rank approximation of the causal structure. It forces the model to learn fundamental interaction patterns that are reused across different feature channels.
\subsubsection{Graph Propagation and Loss Functions}
The latent representation $\mathbf{Z}$ is propagated through this learned structure:
\begin{equation}
\mathbf{Z}_{next} = \tanh(\mathbf{Z} \cdot \mathbf{W}_{eff})
\end{equation}
The training is guided by a composite loss function $\mathcal{L}_{total}$ that balances reconstruction accuracy, prediction fidelity, and structural sparsity:
\begin{equation}
\mathcal{L}_{total} = \underbrace{\frac{1}{T}\sum_{t} ||x_t - \hat{x}_t^{rec}||^2}_{\text{Reconstruction Loss}} + \underbrace{\frac{1}{T}\sum_{t} ||x_{t+1} - \hat{x}_{t+1}^{pred}||^2}_{\text{Prediction Loss}} + \lambda_{L1} \underbrace{||\mathbf{A}_{learned}||_1}_{\text{Sparsity Loss}}
\end{equation}
\begin{itemize}
    \item \textbf{Prediction Loss}: Directly implements the Neural Granger Causality principle—if node $j$ causes node $i$, the link $A_{ji}$ must be non-zero to minimize the prediction error of $x_{i, t+1}$.
    \item \textbf{Sparsity Loss}: An L1 penalty on the adjacency matrix $\mathbf{A}$ promotes a sparse graph, reflecting the assumption that physical systems are locally connected and not fully dense.
\end{itemize}
\subsection{Dynamic Inference and Jacobian Analysis}
While the learned matrix $\mathbf{A}$ represents the ``static'' or average structural connectivity, real-world systems exhibit dynamic causality where interaction strengths fluctuate over time. GeoDCD extracts these dynamics during the inference phase.
\subsubsection{Jacobian Sensitivity Analysis}
To quantify the instantaneous causal strength from node $j$ to node $i$ at time $t$, we compute the Jacobian of the predicted future state with respect to the input state.
Let $\hat{x}_{i, t+1} = f(\mathbf{x}_t)$ be the model's prediction. The dynamic causal strength is defined as:
\begin{equation}
\mathcal{S}_{i \leftarrow j}(t) = \left\| \frac{\partial \hat{x}_{i, t+1}}{\partial x_{j, t}} \right\|_F
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm, aggregating the sensitivity across the feature dimension. We compute this via automatic differentiation, iterating through each target node $i$ to obtain the gradient of its prediction with respect to the input tensor $\mathbf{X}_t$. This yields a dynamic adjacency tensor of shape $(N \times N \times T)$, capturing the time-varying causal topology.

\subsection{Experimental Design}
Begin with a section titled Experimental Design describing the objectives and design of the study as well as prespecified components. 

\subsection{Statistical Analysis}
If applicable, include a section titled Statistical Analysis that fully describes the statistical methods with enough detail to enable a knowledgeable reader with access to the original data to verify the results. The values for N, P, and the specific statistical test performed for each experiment should be included in the appropriate figure legend or main text. 

\subsection{Ethical Statements}
For investigations on humans, a statement must be including indicating that informed consent was obtained after the nature and possible consequences of the study was explained.

For authors using experimental animals, a statement must be included indicating that the animals’ care was in accordance with institutional guidelines.

\section*{Acknowledgments}
Anyone who made a contribution to the research or manuscript, but who is not a listed author, should be acknowledged (with their permission). Types of acknowledgements include:

\subsection*{General} 
Thank others for any contributions, whether it be direct technical help or indirect assistance 

\subsection*{Author Contributions} 
Describe contributions of each author to the paper, using the first initial and full last name. 

\medskip Examples:

``S. Zhang conceived the idea and designed the experiments.''

``E. F. Mustermann and J. F. Smith conducted the experiments.''

``All authors contributed equally to the writing of the manuscript.''

\subsection*{Funding}
Name financially supporting bodies (written out in full), followed by the funding awardee and associated grant numbers (if applicable) in square brackets. 

\medskip Example: 

``This work was supported by the Engineering and Physical Sciences Research Council [grant numbers xxxx, yyyy]; the National Science Foundation [grant number zzzz]; and a Leverhulme Trust Research Project Grant.'' 

\medskip
If the research did not receive specific funding, but was performed as part of the employment of the authors, please name this employer. If the funder was involved in the manuscript writing, editing, approval, or decision to publish, please declare this.

\subsection*{Conflicts of Interest}
Conflicts of interest (COIs, also known as ``competing interests'') occur when issues outside research could be reasonably perceived to affect the neutrality or objectivity of the work or its assessment. 

Authors must declare all potential interests – whether or not they actually had an influence – in a ‘Conflicts of Interest’ section, which should explain why the interest may be a conflict. Authors must declare current or recent funding (including for Article Processing Charges) and other payments, goods or services that might influence the work. All funding, whether a conflict or not, must be declared in a ``Funding Statement.'' The involvement of anyone other than the authors who 1) has an interest in the outcome of the work; 2) is affiliated to an organization with such an interest; or 3) was employed or paid by a funder, in the commissioning, conception, planning, design, conduct, or analysis of the work, the preparation or editing of the manuscript, or the decision to publish must be declared.

If there are none, the authors should state ``The author(s) declare(s) that there is no conflict of interest regarding the publication of this article.'' Submitting authors are responsible for coauthors declaring their interests. Declared conflicts of interest will be considered by the editor and reviewers and included in the published article.

\subsection*{Data Availability}
A data availability statement is compulsory for all research articles. This statement describes whether and how others can access the data supporting the findings of the paper, including 1) what the nature of the data is, 2) where the data can be accessed, and 3) any restrictions on data access and why.

If data are in an archive, include the accession number or a placeholder for it. Also include any materials that must be obtained through a Material Transfer Agreements (MTA). 

\section*{Supplementary Materials}
Describe any supplementary materials submitted with the manuscript (e.g., audio files, video clips or datasets). 

Please group supplementary materials in the following order: materials and methods, figures, tables, and other files (such as movies, data, interactive images, or database files). 

\medskip Example:
Fig. S1. Title of the first supplementary figure.

Fig. S2. Title of the second supplementary figure.

Table S1. Title of the first supplementary table.

Data file S1. Title of the first supplementary data file.

Movie S1. Title of the first supplementary movie.

\medskip
Be sure to submit all supplementary materials with the manuscript and remember to reference the supplementary materials at appropriate points within the manuscript. We recommend citing specific items, rather than referring to the supplementary materials in general, for example: ``See Figures S1-S10 in the Supplementary Material for comprehensive image analysis.''

A link to access the supplementary materials will be provided in the published article.

Supplementary Materials may include additional author notes—for example, a list of group authors.

\section*{Guidelines for References}
References may be submitted in any style. If accepted, Research will reformat the references. Authors are responsible for ensuring that the information in each reference is complete and accurate. All data must be cited and references to “data not shown” or citations to unpublished results are permitted.

There is only one reference list for all sources cited in the main text, figure and table legends, and Supplementary Materials. Do not include a second reference list in the Supplementary Materials section. Include references cited only in the Supplementary Materials at the end of the reference section of the main text; reference numbering should continue as if the Supplementary Materials are a continuation of the main text. References cited only in the Supplementary Materials section are not counted toward length guidelines.

Please do not include any extraneous language such as explanatory notes as part of a reference to a given source. Research prefers that manuscripts do not include end notes; if information is important enough to include, please put into main text.  If you need to include notes, please explain why they are needed in your cover letter to the editor.

\printbibliography

\end{document}
