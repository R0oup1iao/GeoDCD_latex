

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\graphicspath{ {./figures/} }
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{lineno}
\linenumbers


%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.
\usepackage[style=nejm, 
citestyle=numeric-comp,
sorting=none]{biblatex}
\addbibresource{sample.bib}

%%%%%% Title %%%%%%
% Full titles can be a maximum of 200 characters, including spaces. 
% Title Format: Use title case, capitalizing the first letter of each word, except for certain small words, such as articles and short prepositions
\title{Research \LaTeX\ Template}

%%%%%% Authors %%%%%%
% Authors should be listed in order of contribution to the paper, by full first name, then middle initial (if any), followed by last name and separated by commas.
% Please do not use initials for first names. If you use your middle name as a full name, use an initial for the first name and spell out your full middle name.
% Use a superscript asterisk (*) to identify the corresponding author and be sure to include that person’s e-mail address. Use symbols (in this order: †, ‡, §, ||, ¶, #, ††, ‡‡, etc.) for author notes, such as present addresses, “These authors contributed equally to this work” notations, and similar information.
% You can include group authors, but please include a list of the actual authors (the group members) in the Supplementary Materials.
\author[1*$\dag$]{Author One}
\author[2$\dag$]{Author Two}
\author[2]{Author Three}
\author[1,2]{Author Four}

%%%%%% Affiliations %%%%%%
\affil[1]{Department of Physics, A University, City, Country.}
\affil[2]{Department of Astronomy, B University, City, Country.}
\affil[*]{Address correspondence to: email@email.com}
\affil[$\dag$]{These authors contributed equally to this work.}

%%%%%% Date %%%%%%
% Date is optional
\date{}

%%%%%% Spacing %%%%%%
% Use paragraph spacing of 1.5 or 2 (for double spacing, use command \doublespacing)
\onehalfspacing

\begin{document}

\maketitle

%%%%%% Abstract %%%%%%
\begin{abstract}
The abstract should be a single paragraph written in plain language that a general reader can understand. Do not include citations, figures, tables, or undefined abbreviations in the abstract. Any abbreviations that appear in the title should be defined in the abstract. The length should be 200 words and not exceed 300 words, to include: 
\begin{itemize}
    \item An opening sentence that states the question/problem addressed by the research AND
    \item Enough background content to give context to the study AND
    \item A brief statement of primary results AND
    \item A short concluding sentence.
\end{itemize} 
\end{abstract}

%%%%%% Main Text %%%%%%

\section{Introduction}

\section{Results}
The results should describe the experiments performed and the findings observed. The results section should be divided into subsections to delineate different experimental themes. 
\begin{itemize}
    \item All data should be presented in the Results. No data should be presented for the first time in the Discussion. Data (such as from Western blots) should be appropriately quantified.
    \item Subheadings must be either all complete sentences or all phrases. They should be brief, ideally less than 10 words. Subheadings should not end in a period. Your paper may have as many subheadings as are necessary.
    \item Figures and tables must be called out in numerical order. For example, the first mention of any panel of Fig. 3 cannot precede the first mention of all panels of Fig. 2. The supplementary figures (for example, fig. S1) and tables (table S1) must also be called out in numerical order. 
\end{itemize}

\section{Discussion}
Include a Discussion that summarizes (but does not merely repeat) your conclusions and elaborates on their implications. There should be a paragraph outlining the limitations of your results and interpretation, as well as a discussion of the steps that need to be taken for the findings to be applied. Please avoid claims of priority. 

\section{Preliminary}
\label{sec:preliminary}

This section establishes the theoretical underpinnings of our study by tracing the evolution of Granger Causality from linear formulations to neural interpretations. We subsequently provide a formal definition of the multivariate spatiotemporal causal discovery problem, grounded in geometric coordinates.

\subsection{Granger Causality: Linear and Neural Views}

\paragraph{Linear Granger Causality}
The core tenet of Granger Causality lies in \textit{predictability}. For two time-series variables $X_j$ and $X_i$, $X_j$ is said to Granger-cause $X_i$ ($X_j \to X_i$) if the inclusion of $X_j$'s history significantly reduces the prediction error for $X_i$'s future values, compared to using the history of $X_i$ alone.

In classical Vector Autoregression (VAR) models, this relationship is formalized as:
\begin{equation}
    \mathbf{X}_t = \sum_{\tau=1}^P \mathbf{A}_\tau \mathbf{X}_{t-\tau} + \varepsilon_t
\end{equation}
where $P$ denotes the maximum time lag, and $\mathbf{A}_\tau$ represents the coefficient matrix at lag $\tau$. If the element $A_{\tau, ij} = 0$ for all $\tau \in \{1, \dots, P\}$, we conclude that $X_j \nrightarrow X_i$.

However, this linear formulation faces two significant impediments when applied to large-scale geospatial systems ($N \sim 10^3$). First, the parameter space expands quadratically ($O(N^2)$), rendering computation intractable. Second, linear assumptions fail to capture the complex, nonlinear dynamics inherent in physical phenomena, such as fluid turbulence or nonlinear climatic interactions.

\paragraph{Neural Granger Causality}
To surmount the limitations of linearity, Neural Granger Causality (Neural GC) replaces linear transformations with nonlinear deep neural networks, denoted as $f_\theta$:
\begin{equation}
    \mathbf{X}_t = f_\theta(\mathbf{X}_{t-1}, \dots, \mathbf{X}_{t-P}) + \varepsilon_t
\end{equation}
In this framework, causal inference does not rely on fixed coefficient matrices. Instead, causality is estimated by examining how perturbations in the input history influence the model output. Jacobian-based sensitivity analysis is the prevailing method for quantifying this influence:
\begin{equation}
    \mathcal{S}_{i\leftarrow j}(t) = \left| \frac{\partial \hat{x}_{i, t+1}}{\partial x_{j, t}} \right|
\end{equation}
GeoDCD operates within this Neural GC paradigm, specifically adapted and optimized for high-dimensional geometric spatiotemporal data.

\subsection{Problem Definition}
We consider a multivariate spatiotemporal system comprising $N$ spatial locations. The available data consists of two distinct components:

\begin{itemize}
    \item \textbf{Observation Matrix} $\mathbf{D} \in \mathbb{R}^{T \times N}$: Here, $T$ denotes the total time steps and $N$ the number of spatial nodes. At time $t$, the system state vector is denoted as $\mathbf{x}_t = \mathbf{D}_{t, :} \in \mathbb{R}^N$.
    \item \textbf{Coordinate Matrix} $\mathbf{C} \in \mathbb{R}^{N \times 2}$ (or $\mathbb{R}^{N \times 3}$): This contains the physical positioning of each variable (e.g., latitude/longitude or Cartesian coordinates), providing the model with critical geometric priors.
\end{itemize}

Our objective is to leverage $\mathbf{D}$ and $\mathbf{C}$ to infer latent causal structures, specifically:
\begin{enumerate}
    \item \textbf{Static Causal Structure} $\mathbf{A} \in \mathbb{R}^{N \times N}$: The directed connectivity skeleton representing persistent interactions that do not vanish over time.
    \item \textbf{Dynamic Causal Strength} $\mathcal{S}(t) \in \mathbb{R}^{N \times N}, \quad t=1, \dots, T$: A metric capturing the instantaneous intensity of interactions as the system state evolves.
\end{enumerate}

Geospatial systems are typically characterized by high dimensionality ($N \gg T$), strong spatial autocorrelation, and non-stationarity. GeoDCD addresses these challenges through geometrically aware hierarchical modeling.

\section{Methodology}
\subsection{Overall Workflow Architecture}
The GeoDCD framework operates as a unified, end-to-end deep learning pipeline. It accepts raw multivariate time-series data and their associated spatial coordinates as input and produces both a static causal graph (representing long-term structural dependencies) and a dynamic causal tensor (representing instantaneous interaction strengths). The methodology is structured into five sequential components, each addressing a specific aspect of the spatiotemporal causal discovery problem.
The pipeline is composed of five sequential components: (1) \textbf{Spatiotemporal Data Ingestion}, which handles preprocessing and tensor construction; (2) \textbf{Hierarchical Geometric Pooling}, performing bottom-up spatial aggregation; (3) \textbf{Geo-Encoder-Decoder Backbone}, modeling latent dynamics via Transformers; (4) \textbf{Sparse Structural Learning}, estimating the adjacency matrix via Basis Decomposition; and (5) \textbf{Dynamic Inference \& Jacobian Analysis}, extracting time-varying causal graphs.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/overview.png}
    \caption{Overall Workflow of GeoDCD. Top: Raw grid data. Middle: The Hierarchy (Level 0 $\to$ Level 1) showing nodes merging into patches. Bottom: The Network Architecture (Encoder $\to$ Graph Layer $\to$ Decoder). Arrows indicate the forward pass.}
    \label{fig:workflow}
\end{figure}
\subsection{Spatiotemporal Data Ingestion and Preprocessing}
The foundation of the pipeline is the rigorous preparation of data tensors. We employ a sliding window strategy to model temporal dependencies. The multivariate time-series $\mathbf{D} \in \mathbb{R}^{T_{total} \times N}$ is sliced into overlapping sequences of length $L$, transforming the dataset into a tensor $\mathcal{X} \in \mathbb{R}^{B \times N \times L}$. This allows the model to learn from local temporal contexts.
To facilitate stable training and geometric clustering, we apply Z-score normalization (standardization) to both the time-series data and the spatial coordinates $\mathbf{C} \in \mathbb{R}^{N \times 2}$. Normalizing coordinates ensures that the clustering process is invariant to the absolute scale of the physical system.
\subsection{Hierarchical Geometric Pooling}
To resolve the $O(N^2)$ scalability bottleneck, GeoDCD introduces a Geometric Pooler. This component uses physical proximity to aggregate nodes into ``super-nodes'' or patches, creating a hierarchical representation of the system. This allows the causal discovery process to occur at multiple resolutions—coarse-grained for regional trends and fine-grained for local interactions.
\subsubsection{K-Means Spatial Clustering}
We employ the K-Means algorithm on the normalized coordinate matrix $\mathbf{C}$ to partition the $N$ nodes into $K$ distinct clusters. This step injects strong inductive bias: we assume that spatially proximal nodes are more likely to share functional characteristics or be governed by similar local dynamics.
The algorithm minimizes the intra-cluster variance:
\begin{equation}
\arg \min_{\mathbf{S}} \sum_{k=1}^K \sum_{i \in \text{Cluster}_k} || \mathbf{c}_i - \mathbf{\mu}_k ||^2
\end{equation}
where $\mathbf{S} \in \{0, 1\}^{N \times K}$ is the hard assignment matrix derived from the clustering labels. Specifically, $S_{ik} = 1$ if node $i$ is assigned to cluster $k$, and 0 otherwise. This matrix is precomputed during the initialization phase.
\subsubsection{Feature Aggregation (Pooling)}
During the forward pass, the feature representations of nodes within a cluster are aggregated to form the input for the next level of the hierarchy. The pooling operation is defined as a matrix multiplication:
\begin{equation}
\mathbf{X}^{(l+1)} = (\mathbf{S}^{(l)})^\top \mathbf{X}^{(l)}
\end{equation}
To prevent magnitude explosion, the assignment matrix $\mathbf{S}$ is column-normalized, effectively computing the average feature vector for each cluster. This reduces the spatial dimension from $N$ to $K$, significantly lowering the computational cost for subsequent layers. The coordinates for the new level are similarly updated by computing the centroids of the clusters: $\mathbf{C}^{(l+1)} = (\mathbf{S}^{(l)})^\top \mathbf{C}^{(l)}$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pooling.png}
    \caption{Geometric Pooling Visualization. A diagram showing a set of scattered points (nodes) on a 2D plane. Circles delineate K-Means clusters. An arrow points to a smaller set of nodes representing the cluster centroids, illustrating the dimensionality reduction.}
    \label{fig:pooling}
\end{figure}
\subsection{Geo-Encoder-Decoder Backbone}
At the heart of GeoDCD is a sequence-to-sequence architecture that models the temporal evolution of the system. We utilize a Causal Transformer, adapted from the standard Transformer architecture, to capture long-range temporal dependencies without suffering from the vanishing gradient problem common in Recurrent Neural Networks (RNNs).
\subsubsection{Positional Encoding}
Since Transformers are permutation-invariant, we must inject explicit temporal order information. We use sinusoidal positional encodings added to the input embeddings:
\begin{align}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{model}})
\end{align}
This allows the model to distinguish between different time steps within the look-back window.
\subsubsection{Causal Self-Attention}
The core mechanism is the Multi-Head Self-Attention (MSA). To ensure that the prediction for time $t$ depends only on the history $t' \le t$ (respecting physical causality), we apply a Causal Mask to the attention scores.
The attention mechanism is computed as:
\begin{equation}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{M}\right)\mathbf{V}
\end{equation}
Here, $\mathbf{M}$ is a square matrix where $M_{ij} = -\infty$ if $j > i$ (masking future positions) and $0$ otherwise. This strictly prevents information leakage from the future.
\subsubsection{Pre-LayerNorm Architecture}
To improve training stability and convergence, we adopt the \textbf{Pre-LayerNorm} architecture. Unlike the original Post-LN design where Layer Normalization (LN) is applied after the residual connection, we apply LN before the Multi-Head Self-Attention (MSA) and Feed-Forward Networks (FFN). This structural modification creates a direct path for gradients to flow through the residual connections, significantly mitigating the risk of gradient vanishing in deep networks.
The forward pass for a single encoder layer is defined as:
\begin{align}
\mathbf{Z}' &= \text{MSA}(\text{LN}(\mathbf{Z})) + \mathbf{Z} \\
\mathbf{Z}_{out} &= \text{FFN}(\text{LN}(\mathbf{Z}')) + \mathbf{Z}'
\end{align}
This design mitigates the risk of gradient vanishing in deep networks. The architecture acts as a temporal autoencoder:
\begin{enumerate}
    \item \textbf{Encoder}: Maps the input sequence $\mathbf{X}$ to a latent dynamic representation $\mathbf{Z} \in \mathbb{R}^{B \times N \times L \times D_{model}}$.
    \item \textbf{Decoder}: Receives the processed latent states and attempts to reconstruct the input sequence (Reconstruction) and predict the next time step (Forecasting).
\end{enumerate}
\subsection{Sparse Structural Learning via Basis Decomposition}
A standard dense layer connecting $N$ nodes would require $N^2$ parameters, which is prohibitive. GeoDCD implements a novel Causal Graph Layer that learns the adjacency matrix efficiently.
\subsubsection{Basis Decomposition}
Instead of learning a unique weight for every pair of nodes directly, we decompose the weight tensor into a linear combination of a small number of shared basis matrices. Let $N_{basis}$ be the number of bases (e.g., 4) and $D_{model}$ be the channel dimension.
We define:
\begin{itemize}
    \item $\mathbf{B} \in \mathbb{R}^{N_{basis} \times N \times N}$: The tensor of basis adjacency matrices.
    \item $\mathbf{c} \in \mathbb{R}^{D_{model} \times N_{basis}}$: The coefficient matrix determining how channels mix the bases.
\end{itemize}
The effective weight matrix for the graph convolution is reconstructed as:
\begin{equation}
\mathbf{W}_{eff} = \sum_{k=1}^{N_{basis}} \mathbf{c}_{\cdot, k} \otimes \mathbf{B}_{k}
\end{equation}
This reduces the parameter space significantly, acting as a low-rank approximation of the causal structure. It forces the model to learn fundamental interaction patterns that are reused across different feature channels.
\subsubsection{Graph Propagation and Loss Functions}
The latent representation $\mathbf{Z}$ is propagated through this learned structure:
\begin{equation}
\mathbf{Z}_{next} = \tanh(\mathbf{Z} \cdot \mathbf{W}_{eff})
\end{equation}
The training is guided by a composite loss function $\mathcal{L}_{total}$ that balances reconstruction accuracy, prediction fidelity, and structural sparsity:
\begin{equation}
\mathcal{L}_{total} = \underbrace{\frac{1}{T}\sum_{t} ||x_t - \hat{x}_t^{rec}||^2}_{\text{Reconstruction Loss}} + \underbrace{\frac{1}{T}\sum_{t} ||x_{t+1} - \hat{x}_{t+1}^{pred}||^2}_{\text{Prediction Loss}} + \lambda_{L1} \underbrace{||\mathbf{A}_{learned}||_1}_{\text{Sparsity Loss}}
\end{equation}
\begin{itemize}
    \item \textbf{Prediction Loss}: Directly implements the Neural Granger Causality principle—if node $j$ causes node $i$, the link $A_{ji}$ must be non-zero to minimize the prediction error of $x_{i, t+1}$.
    \item \textbf{Sparsity Loss}: An L1 penalty on the adjacency matrix $\mathbf{A}$ promotes a sparse graph, reflecting the assumption that physical systems are locally connected and not fully dense.
\end{itemize}
\subsection{Dynamic Inference and Jacobian Analysis}
While the learned matrix $\mathbf{A}$ represents the ``static'' or average structural connectivity, real-world systems exhibit dynamic causality where interaction strengths fluctuate over time. GeoDCD extracts these dynamics during the inference phase.
\subsubsection{Jacobian Sensitivity Analysis}
To quantify the instantaneous causal strength from node $j$ to node $i$ at time $t$, we compute the Jacobian of the predicted future state with respect to the input state.
Let $\hat{x}_{i, t+1} = f(\mathbf{x}_t)$ be the model's prediction. The dynamic causal strength is defined as:
\begin{equation}
\mathcal{S}_{i \leftarrow j}(t) = \left\| \frac{\partial \hat{x}_{i, t+1}}{\partial x_{j, t}} \right\|_F
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm, aggregating the sensitivity across the feature dimension. We compute this via automatic differentiation, iterating through each target node $i$ to obtain the gradient of its prediction with respect to the input tensor $\mathbf{X}_t$. This yields a dynamic adjacency tensor of shape $(N \times N \times T)$, capturing the time-varying causal topology.

\subsection{Experimental Design}
Begin with a section titled Experimental Design describing the objectives and design of the study as well as prespecified components. 

\subsection{Statistical Analysis}
If applicable, include a section titled Statistical Analysis that fully describes the statistical methods with enough detail to enable a knowledgeable reader with access to the original data to verify the results. The values for N, P, and the specific statistical test performed for each experiment should be included in the appropriate figure legend or main text. 

\subsection{Ethical Statements}
For investigations on humans, a statement must be including indicating that informed consent was obtained after the nature and possible consequences of the study was explained.

For authors using experimental animals, a statement must be included indicating that the animals’ care was in accordance with institutional guidelines.

\section*{Acknowledgments}
Anyone who made a contribution to the research or manuscript, but who is not a listed author, should be acknowledged (with their permission). Types of acknowledgements include:

\subsection*{General} 
Thank others for any contributions, whether it be direct technical help or indirect assistance 

\subsection*{Author Contributions} 
Describe contributions of each author to the paper, using the first initial and full last name. 

\medskip Examples:

``S. Zhang conceived the idea and designed the experiments.''

``E. F. Mustermann and J. F. Smith conducted the experiments.''

``All authors contributed equally to the writing of the manuscript.''

\subsection*{Funding}
Name financially supporting bodies (written out in full), followed by the funding awardee and associated grant numbers (if applicable) in square brackets. 

\medskip Example: 

``This work was supported by the Engineering and Physical Sciences Research Council [grant numbers xxxx, yyyy]; the National Science Foundation [grant number zzzz]; and a Leverhulme Trust Research Project Grant.'' 

\medskip
If the research did not receive specific funding, but was performed as part of the employment of the authors, please name this employer. If the funder was involved in the manuscript writing, editing, approval, or decision to publish, please declare this.

\subsection*{Conflicts of Interest}
Conflicts of interest (COIs, also known as ``competing interests'') occur when issues outside research could be reasonably perceived to affect the neutrality or objectivity of the work or its assessment. 

Authors must declare all potential interests – whether or not they actually had an influence – in a ‘Conflicts of Interest’ section, which should explain why the interest may be a conflict. Authors must declare current or recent funding (including for Article Processing Charges) and other payments, goods or services that might influence the work. All funding, whether a conflict or not, must be declared in a ``Funding Statement.'' The involvement of anyone other than the authors who 1) has an interest in the outcome of the work; 2) is affiliated to an organization with such an interest; or 3) was employed or paid by a funder, in the commissioning, conception, planning, design, conduct, or analysis of the work, the preparation or editing of the manuscript, or the decision to publish must be declared.

If there are none, the authors should state ``The author(s) declare(s) that there is no conflict of interest regarding the publication of this article.'' Submitting authors are responsible for coauthors declaring their interests. Declared conflicts of interest will be considered by the editor and reviewers and included in the published article.

\subsection*{Data Availability}
A data availability statement is compulsory for all research articles. This statement describes whether and how others can access the data supporting the findings of the paper, including 1) what the nature of the data is, 2) where the data can be accessed, and 3) any restrictions on data access and why.

If data are in an archive, include the accession number or a placeholder for it. Also include any materials that must be obtained through a Material Transfer Agreements (MTA). 

\section*{Supplementary Materials}
Describe any supplementary materials submitted with the manuscript (e.g., audio files, video clips or datasets). 

Please group supplementary materials in the following order: materials and methods, figures, tables, and other files (such as movies, data, interactive images, or database files). 

\medskip Example:
Fig. S1. Title of the first supplementary figure.

Fig. S2. Title of the second supplementary figure.

Table S1. Title of the first supplementary table.

Data file S1. Title of the first supplementary data file.

Movie S1. Title of the first supplementary movie.

\medskip
Be sure to submit all supplementary materials with the manuscript and remember to reference the supplementary materials at appropriate points within the manuscript. We recommend citing specific items, rather than referring to the supplementary materials in general, for example: ``See Figures S1-S10 in the Supplementary Material for comprehensive image analysis.''

A link to access the supplementary materials will be provided in the published article.

Supplementary Materials may include additional author notes—for example, a list of group authors.

\section*{Guidelines for References}
References may be submitted in any style. If accepted, Research will reformat the references. Authors are responsible for ensuring that the information in each reference is complete and accurate. All data must be cited and references to “data not shown” or citations to unpublished results are permitted.

There is only one reference list for all sources cited in the main text, figure and table legends, and Supplementary Materials. Do not include a second reference list in the Supplementary Materials section. Include references cited only in the Supplementary Materials at the end of the reference section of the main text; reference numbering should continue as if the Supplementary Materials are a continuation of the main text. References cited only in the Supplementary Materials section are not counted toward length guidelines.

Please do not include any extraneous language such as explanatory notes as part of a reference to a given source. Research prefers that manuscripts do not include end notes; if information is important enough to include, please put into main text.  If you need to include notes, please explain why they are needed in your cover letter to the editor.

\printbibliography

\end{document}
